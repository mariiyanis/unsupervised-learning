{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c830de2c",
      "metadata": {
        "id": "c830de2c"
      },
      "source": [
        "# Clusterização Hierárquica\n",
        "\n",
        "## 1. Introdução\n",
        "\n",
        "Neste notebook, exploraremos os algoritmos de clusterização hierárquica, uma família de métodos que constrói uma hierarquia de clusters organizando os dados em uma estrutura semelhante a uma árvore. Diferentemente do K-Means, que requer que especifiquemos o número de clusters antecipadamente, a clusterização hierárquica nos permite descobrir a estrutura natural dos dados em diferentes níveis de granularidade.\n",
        "\n",
        "A clusterização hierárquica pode ser dividida em duas abordagens principais:\n",
        "- **Aglomerativa (Bottom-up)**: Inicia com cada ponto como um cluster individual e, iterativamente, combina os clusters mais próximos até formar um único cluster.\n",
        "- **Divisiva (Top-down)**: Inicia com todos os pontos em um único cluster e, recursivamente, divide os clusters até que cada ponto forme seu próprio cluster.\n",
        "\n",
        "### Conteúdos abordados:\n",
        "\n",
        "* **Fundamentação Matemática**: Métricas de distância entre clusters e critérios de ligação.\n",
        "* **Implementação em Python**: Construção do algoritmo aglomerativo passo a passo usando NumPy.\n",
        "* **Dendrogramas**: Visualização da hierarquia de clusters.\n",
        "* **Critérios de Ligação**: Single, Complete, Average e Ward.\n",
        "* **Determinação do Número de Clusters**: Métodos para \"cortar\" o dendrograma.\n",
        "* **Aplicações Práticas**: Análise de dados reais e comparação com K-Means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4924bd0",
      "metadata": {
        "id": "f4924bd0"
      },
      "outputs": [],
      "source": [
        "# Célula de importação de bibliotecas\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Estilo para os gráficos\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a2c8e8",
      "metadata": {
        "id": "44a2c8e8"
      },
      "source": [
        "## 2. Fundamentação Matemática da Clusterização Hierárquica\n",
        "\n",
        "A clusterização hierárquica aglomerativa funciona com base em uma **matriz de distâncias** entre todos os pares de pontos (ou clusters). O algoritmo segue estes passos fundamentais:\n",
        "\n",
        "1. **Inicialização**: Cada observação $\\mathbf{x}_i$ forma um cluster individual $C_i = \\{\\mathbf{x}_i\\}$.\n",
        "\n",
        "2. **Cálculo da Matriz de Distâncias**: Para $N$ pontos, calculamos uma matriz simétrica $D \\in \\mathbb{R}^{N \\times N}$ onde $D_{ij}$ representa a distância entre os pontos $\\mathbf{x}_i$ e $\\mathbf{x}_j$:\n",
        "   $$ D_{ij} = d(\\mathbf{x}_i, \\mathbf{x}_j) $$\n",
        "\n",
        "3. **Iteração**: Em cada passo, encontramos o par de clusters $(C_i, C_j)$ com menor distância e os combinamos em um novo cluster $C_{ij} = C_i \\cup C_j$.\n",
        "\n",
        "4. **Atualização**: Recalculamos as distâncias do novo cluster para todos os outros clusters existentes.\n",
        "\n",
        "5. **Terminação**: O processo continua até que reste apenas um cluster contendo todas as observações."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c410c1a",
      "metadata": {
        "id": "4c410c1a"
      },
      "source": [
        "### Critérios de Ligação (Linkage)\n",
        "\n",
        "O ponto crucial da clusterização hierárquica é como definimos a distância entre dois clusters. Existem vários critérios de ligação:\n",
        "\n",
        "1. **Single Linkage (Ligação Simples)**:\n",
        "   $$ d(C_i, C_j) = \\min_{\\mathbf{x} \\in C_i, \\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
        "   A distância é definida pelos pontos mais próximos entre os clusters.\n",
        "\n",
        "2. **Complete Linkage (Ligação Completa)**:\n",
        "   $$ d(C_i, C_j) = \\max_{\\mathbf{x} \\in C_i, \\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
        "   A distância é definida pelos pontos mais distantes entre os clusters.\n",
        "\n",
        "3. **Average Linkage (Ligação Média)**:\n",
        "   $$ d(C_i, C_j) = \\frac{1}{|C_i||C_j|} \\sum_{\\mathbf{x} \\in C_i} \\sum_{\\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
        "   A distância é a média de todas as distâncias entre pares de pontos dos clusters.\n",
        "\n",
        "4. **Ward Linkage (Critério de Ward)**:\n",
        "   $$d(C_i, C_j) = \\frac{|C_i||C_j|}{|C_i|+|C_j|} \\|\\mathbf{m}_i - \\mathbf{m}_j\\|^2$$\n",
        "   Onde $\\mathbf{m}_i$ e $\\mathbf{m}_j$ são os centróides dos clusters $C_i$ e $C_j$, respectivamente, e $|C_k|$ é o número de pontos no cluster $C_k$.\n",
        "   Minimiza a variância intra-cluster ao combinar clusters. É baseado na soma dos quadrados das distâncias aos centróides."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8a1558",
      "metadata": {
        "id": "fa8a1558"
      },
      "source": [
        "## 3. Preparação dos Dados\n",
        "\n",
        "Vamos começar com um exemplo simples usando dados sintéticos para entender visualmente como funciona a clusterização hierárquica. Depois, aplicaremos o algoritmo ao dataset Iris."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dfb55f3",
      "metadata": {
        "id": "9dfb55f3"
      },
      "outputs": [],
      "source": [
        "# Gerando dados sintéticos simples para demonstração\n",
        "np.random.seed(42)\n",
        "X_simple = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
        "\n",
        "# Visualizar os dados\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_simple[:, 0], X_simple[:, 1], c='blue', s=100, alpha=0.7)\n",
        "for i, (x, y) in enumerate(X_simple):\n",
        "    plt.annotate(f'P{i}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
        "plt.title('Dataset Simples para Demonstração')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Coordenadas dos pontos:\")\n",
        "for i, point in enumerate(X_simple):\n",
        "    print(f\"P{i}: {point}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c51ff83",
      "metadata": {
        "id": "9c51ff83"
      },
      "source": [
        "## 4. Implementação do Algoritmo Hierárquico Aglomerativo\n",
        "\n",
        "Vamos construir uma implementação simplificada do algoritmo hierárquico aglomerativo para entender seus passos fundamentais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2991751b",
      "metadata": {
        "id": "2991751b"
      },
      "outputs": [],
      "source": [
        "class HierarchicalClustering:\n",
        "    def __init__(self, linkage='single'):\n",
        "        \"\"\"\n",
        "        Inicializa o algoritmo de clusterização hierárquica.\n",
        "\n",
        "        Parameters:\n",
        "        linkage: str, critério de ligação ('single', 'complete', 'average')\n",
        "        \"\"\"\n",
        "        self.linkage = linkage\n",
        "        self.merge_history = []\n",
        "        self.distances = []\n",
        "\n",
        "    def _calculate_distance_matrix(self, X):\n",
        "        \"\"\"\n",
        "        Calcula a matriz de distâncias entre todos os pares de pontos.\n",
        "        \"\"\"\n",
        "        n = len(X)\n",
        "        dist_matrix = np.zeros((n, n))\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                dist = np.linalg.norm(X[i] - X[j])\n",
        "                dist_matrix[i, j] = dist\n",
        "                dist_matrix[j, i] = dist\n",
        "\n",
        "        return dist_matrix\n",
        "\n",
        "    def _cluster_distance(self, cluster1, cluster2, X, dist_matrix):\n",
        "        \"\"\"\n",
        "        Calcula a distância entre dois clusters baseado no critério de ligação.\n",
        "        \"\"\"\n",
        "        if self.linkage == 'single':\n",
        "            # Distância mínima entre qualquer par de pontos dos clusters\n",
        "            min_dist = float('inf')\n",
        "            for i in cluster1:\n",
        "                for j in cluster2:\n",
        "                    if dist_matrix[i, j] < min_dist:\n",
        "                        min_dist = dist_matrix[i, j]\n",
        "            return min_dist\n",
        "\n",
        "        elif self.linkage == 'complete':\n",
        "            # Distância máxima entre qualquer par de pontos dos clusters\n",
        "            max_dist = 0\n",
        "            for i in cluster1:\n",
        "                for j in cluster2:\n",
        "                    if dist_matrix[i, j] > max_dist:\n",
        "                        max_dist = dist_matrix[i, j]\n",
        "            return max_dist\n",
        "\n",
        "        # elif self.linkage == 'average':\n",
        "        # Distância média entre todos os pares de pontos dos clusters\n",
        "        # ...\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Executa o algoritmo de clusterização hierárquica aglomerativa.\n",
        "        \"\"\"\n",
        "        n = len(X)\n",
        "\n",
        "        # Inicializar cada ponto como um cluster\n",
        "        clusters = [[i] for i in range(n)]\n",
        "\n",
        "        # Calcular matriz de distâncias inicial\n",
        "        dist_matrix = self._calculate_distance_matrix(X)\n",
        "\n",
        "        self.merge_history = []\n",
        "        self.distances = []\n",
        "\n",
        "        step = 0\n",
        "        print(f\"Passo inicial: {len(clusters)} clusters individuais\")\n",
        "        print(f\"Clusters: {clusters}\\n\")\n",
        "\n",
        "        # Continuar até que reste apenas um cluster\n",
        "        while len(clusters) > 1:\n",
        "            # Encontrar o par de clusters mais próximo\n",
        "            min_distance = float('inf')\n",
        "            merge_i, merge_j = -1, -1\n",
        "\n",
        "            for i in range(len(clusters)):\n",
        "                for j in range(i+1, len(clusters)):\n",
        "                    distance = self._cluster_distance(clusters[i], clusters[j], X, dist_matrix)\n",
        "                    if distance < min_distance:\n",
        "                        min_distance = distance\n",
        "                        merge_i, merge_j = i, j\n",
        "\n",
        "            # Combinar os clusters mais próximos\n",
        "            new_cluster = clusters[merge_i] + clusters[merge_j]\n",
        "\n",
        "            # Salvar informações da fusão\n",
        "            self.merge_history.append((clusters[merge_i].copy(), clusters[merge_j].copy()))\n",
        "            self.distances.append(min_distance)\n",
        "\n",
        "            step += 1\n",
        "            print(f\"Passo {step}: Combinar clusters {clusters[merge_i]} e {clusters[merge_j]}\")\n",
        "            print(f\"Distância: {min_distance:.3f}\")\n",
        "\n",
        "            # Remover os clusters antigos e adicionar o novo\n",
        "            clusters = [clusters[k] for k in range(len(clusters)) if k != merge_i and k != merge_j]\n",
        "            clusters.append(new_cluster)\n",
        "\n",
        "            print(f\"Clusters restantes: {clusters}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3699be72",
      "metadata": {
        "id": "3699be72"
      },
      "source": [
        "## 5. Executando o Algoritmo no Dataset Simples\n",
        "\n",
        "Vamos aplicar nossa implementação nos dados simples para observar passo a passo como os clusters são formados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d20282",
      "metadata": {
        "id": "d3d20282"
      },
      "outputs": [],
      "source": [
        "# Executar a clusterização hierárquica com ligação simples\n",
        "hc_single = HierarchicalClustering(linkage='single')\n",
        "hc_single.fit(X_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71cc9365",
      "metadata": {
        "id": "71cc9365"
      },
      "source": [
        "## 6. Dendrogramas: Visualizando a Hierarquia\n",
        "\n",
        "Um **dendrograma** é a representação gráfica da hierarquia de clusters. É uma estrutura em forma de árvore que mostra a ordem e as distâncias nas quais os clusters foram combinados.\n",
        "\n",
        "### Interpretação do Dendrograma:\n",
        "- **Eixo horizontal**: Representa as observações ou clusters.\n",
        "- **Eixo vertical**: Representa a distância na qual os clusters foram unidos.\n",
        "- **Altura dos ramos**: Indica a dissimilaridade entre os clusters combinados.\n",
        "\n",
        "Vamos usar a implementação otimizada do SciPy para criar dendrogramas profissionais:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd1ce69",
      "metadata": {
        "id": "6fd1ce69"
      },
      "source": [
        "### Usando as Funções `linkage` e `fcluster` do SciPy\n",
        "\n",
        "O SciPy fornece funções otimizadas para clusterização hierárquica que são muito mais eficientes que nossa implementação educacional. As duas funções principais são:\n",
        "\n",
        "#### 1. Função `linkage(X, method)`\n",
        "\n",
        "A função `linkage` calcula a matriz de ligação que representa a hierarquia de clusters:\n",
        "\n",
        "```python\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "# Sintaxe básica\n",
        "linkage_matrix = linkage(X, method='ward')\n",
        "```\n",
        "\n",
        "**Parâmetros principais:**\n",
        "- `X`: matriz de dados (n_samples × n_features)\n",
        "- `method`: critério de ligação ('single', 'complete', 'average', 'ward')\n",
        "- `metric`: métrica de distância (padrão: 'euclidean')\n",
        "\n",
        "**Retorno:**\n",
        "- Matriz (n-1) × 4 onde cada linha representa uma fusão:\n",
        "  - Colunas 0 e 1: índices dos clusters sendo combinados\n",
        "  - Coluna 2: distância da fusão\n",
        "  - Coluna 3: número de observações no novo cluster\n",
        "\n",
        "#### 2. Função `fcluster(Z, t, criterion)`\n",
        "\n",
        "A função `fcluster` extrai clusters da matriz de ligação com base em um critério de corte:\n",
        "\n",
        "```python\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Obter clusters com base na distância\n",
        "clusters = fcluster(linkage_matrix, t=3.0, criterion='distance')\n",
        "\n",
        "# Obter um número específico de clusters\n",
        "clusters = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
        "```\n",
        "\n",
        "**Parâmetros principais:**\n",
        "- `Z`: matriz de ligação (resultado de `linkage`)\n",
        "- `t`: valor do critério de corte\n",
        "- `criterion`: tipo de critério ('distance', 'maxclust', 'inconsistent')\n",
        "\n",
        "**Critérios de corte:**\n",
        "- `'distance'`: corta em uma altura específica do dendrograma\n",
        "- `'maxclust'`: força um número específico de clusters\n",
        "- `'inconsistent'`: baseado no coeficiente de inconsistência"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95daf311",
      "metadata": {
        "id": "95daf311"
      },
      "outputs": [],
      "source": [
        "# Usando scipy para criar dendrogramas profissionais\n",
        "# Diferentes critérios de ligação\n",
        "\n",
        "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    # Calcular a matriz de ligação\n",
        "    linkage_matrix = linkage(X_simple, method=method)\n",
        "\n",
        "    # Criar o dendrograma\n",
        "    dendrogram(linkage_matrix, ax=axes[i], labels=[f'P{j}' for j in range(len(X_simple))])\n",
        "    axes[i].set_title(f'Dendrograma - {method.capitalize()} Linkage')\n",
        "    axes[i].set_xlabel('Pontos de Dados')\n",
        "    axes[i].set_ylabel('Distância')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681505ad",
      "metadata": {
        "id": "681505ad"
      },
      "source": [
        "### Comparação dos Critérios de Ligação\n",
        "\n",
        "Cada critério de ligação produz diferentes estruturas de cluster:\n",
        "\n",
        "- **Single Linkage**: Tende a criar clusters elongados e pode sofrer do \"efeito corrente\" (chaining effect).\n",
        "- **Complete Linkage**: Produz clusters mais compactos e esféricos.\n",
        "- **Average Linkage**: Um meio-termo entre single e complete.\n",
        "- **Ward Linkage**: Minimiza a variância intra-cluster, similar ao objetivo do K-Means."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4d6ce8",
      "metadata": {
        "id": "af4d6ce8"
      },
      "source": [
        "## 7. Determinando o Número de Clusters\n",
        "\n",
        "Uma das grandes vantagens da clusterização hierárquica é que podemos \"cortar\" o dendrograma em diferentes alturas para obter diferentes números de clusters. Isso é feito traçando uma linha horizontal através do dendrograma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4d6ce8_code",
      "metadata": {
        "id": "af4d6ce8_code"
      },
      "outputs": [],
      "source": [
        "# Usar Ward linkage para o exemplo\n",
        "linkage_matrix = linkage(X_simple, method='ward')\n",
        "\n",
        "# Definir diferentes alturas de corte\n",
        "cut_heights = [2.0, 4.0, 6.0]\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Visualizar o dendrograma com diferentes cortes\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linkage_matrix, labels=[f'P{j}' for j in range(len(X_simple))])\n",
        "\n",
        "for height, color in zip(cut_heights, colors):\n",
        "    plt.axhline(y=height, color=color, linestyle='--', label=f'Corte em {height}')\n",
        "\n",
        "plt.title('Dendrograma com Linhas de Corte')\n",
        "plt.xlabel('Pontos de Dados')\n",
        "plt.ylabel('Distância')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mxozms9yqro",
      "metadata": {
        "id": "mxozms9yqro"
      },
      "outputs": [],
      "source": [
        "# Predição de clusters baseada em diferentes linhas de corte\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "print(\"Predição de clusters para diferentes alturas de corte:\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "for i, height in enumerate(cut_heights):\n",
        "    # Obter clusters para a altura de corte específica\n",
        "    clusters = fcluster(linkage_matrix, height, criterion='distance')\n",
        "    n_clusters = len(np.unique(clusters))\n",
        "\n",
        "    print()\n",
        "    print(f\"Altura de corte: {height}\")\n",
        "    print(f\"Número de clusters: {n_clusters}\")\n",
        "\n",
        "    # Mostrar quais pontos pertencem a cada cluster\n",
        "    for cluster_id in np.unique(clusters):\n",
        "        points = [f\"P{j}\" for j in range(len(X_simple)) if clusters[j] == cluster_id]\n",
        "        print(f\"  Cluster {cluster_id}: {points}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ddf1fa",
      "metadata": {
        "id": "28ddf1fa"
      },
      "outputs": [],
      "source": [
        "# Visualização dos clusters resultantes\n",
        "fig, axes = plt.subplots(1, len(cut_heights), figsize=(15, 4))\n",
        "\n",
        "for i, height in enumerate(cut_heights):\n",
        "    clusters = fcluster(linkage_matrix, height, criterion='distance')\n",
        "    scatter = axes[i].scatter(X_simple[:, 0], X_simple[:, 1], c=clusters, s=100, alpha=0.7, cmap='viridis')\n",
        "\n",
        "    # Adicionar rótulos dos pontos\n",
        "    for j, (x, y) in enumerate(X_simple):\n",
        "        axes[i].annotate(f'P{j}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "    axes[i].set_title(f'Clusters (corte = {height})\\n{len(np.unique(clusters))} clusters')\n",
        "    axes[i].set_xlabel('Feature 1')\n",
        "    axes[i].set_ylabel('Feature 2')\n",
        "    axes[i].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee707fe",
      "metadata": {
        "id": "6ee707fe"
      },
      "source": [
        "## 8. Aplicação ao Dataset Iris\n",
        "\n",
        "Agora vamos aplicar a clusterização hierárquica ao dataset Iris e comparar os resultados com o K-Means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b9b972",
      "metadata": {
        "id": "51b9b972"
      },
      "outputs": [],
      "source": [
        "# Carregar o dataset Iris\n",
        "iris = load_iris()\n",
        "X_iris = iris.data[:, 2:]  # Usar comprimento e largura da pétala\n",
        "y_true = iris.target\n",
        "\n",
        "# Aplicar diferentes métodos de ligação\n",
        "methods = ['ward', 'complete', 'average', 'single']\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    # Calcular a matriz de ligação\n",
        "    linkage_matrix = linkage(X_iris, method=method)\n",
        "\n",
        "    # Dendrograma\n",
        "    dendrogram(linkage_matrix, ax=axes[0, i], no_labels=True)\n",
        "    axes[0, i].set_title(f'Dendrograma - {method.capitalize()}')\n",
        "\n",
        "    # Obter 3 clusters\n",
        "    clusters = fcluster(linkage_matrix, 3, criterion='maxclust')\n",
        "\n",
        "    # Plotar os clusters\n",
        "    scatter = axes[1, i].scatter(X_iris[:, 0], X_iris[:, 1], c=clusters, s=50, alpha=0.7, cmap='viridis')\n",
        "    axes[1, i].set_title(f'Clusters - {method.capitalize()}')\n",
        "    axes[1, i].set_xlabel('Comprimento da Pétala')\n",
        "    axes[1, i].set_ylabel('Largura da Pétala')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc5cdf49",
      "metadata": {
        "id": "dc5cdf49"
      },
      "source": [
        "### Avaliação dos Resultados\n",
        "\n",
        "Vamos calcular a taxa de acertos para cada método de ligação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d064c6",
      "metadata": {
        "id": "e4d064c6"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "def calculate_purity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula a pureza dos clusters comparando com os rótulos verdadeiros.\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    n_samples = len(y_true)\n",
        "\n",
        "    for cluster_id in np.unique(y_pred):\n",
        "        mask = (y_pred == cluster_id)\n",
        "        if np.sum(mask) > 0:\n",
        "            dominant_label = mode(y_true[mask], keepdims=True)[0][0]\n",
        "            correct_predictions += np.sum(y_true[mask] == dominant_label)\n",
        "\n",
        "    return correct_predictions / n_samples\n",
        "\n",
        "print(\"Comparação dos métodos de ligação no dataset Iris:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for method in methods:\n",
        "    linkage_matrix = linkage(X_iris, method=method)\n",
        "    clusters = fcluster(linkage_matrix, 3, criterion='maxclust')\n",
        "    purity = calculate_purity(y_true, clusters)\n",
        "    print(f\"{method.capitalize():12} Linkage: {purity:.1%} de acertos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bf0c30",
      "metadata": {
        "id": "54bf0c30"
      },
      "source": [
        "### Exercício 1: Implementação do Average Linkage\n",
        "\n",
        "Complete a implementação da nossa classe `HierarchicalClustering` adicionando o método **Average Linkage**. Em seguida, teste todos os três métodos de ligação (single, complete, average) no dataset simples (`X_simple`) e compare os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4753c349",
      "metadata": {
        "id": "4753c349"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "X_simple = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
        "\n",
        "class HierarchicalClustering:\n",
        "    def __init__(self, linkage='single'):\n",
        "        self.linkage = linkage\n",
        "        self.merge_history = []\n",
        "        self.distances = []\n",
        "\n",
        "    def _calculate_distance_matrix(self, X):\n",
        "        # euclidiana\n",
        "        n = len(X)\n",
        "        dist_matrix = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                dist = np.linalg.norm(X[i] - X[j])\n",
        "                dist_matrix[i, j] = dist\n",
        "                dist_matrix[j, i] = dist\n",
        "        return dist_matrix\n",
        "\n",
        "    def _cluster_distance(self, cluster1_indices, cluster2_indices, X, dist_matrix):\n",
        "        cluster1_points = X[cluster1_indices]\n",
        "        cluster2_points = X[cluster2_indices]\n",
        "\n",
        "        # calculando a matriz de distâncias entre os pontos dos dois clusters\n",
        "        all_pair_distances = cdist(cluster1_points, cluster2_points, metric='euclidean')\n",
        "\n",
        "        if self.linkage == 'single':\n",
        "            return np.min(all_pair_distances)\n",
        "\n",
        "        elif self.linkage == 'complete':\n",
        "            return np.max(all_pair_distances)\n",
        "\n",
        "        elif self.linkage == 'average':\n",
        "            return np.mean(all_pair_distances)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Linkage method '{self.linkage}' não suportado.\")\n",
        "\n",
        "    def fit(self, X):\n",
        "        n = len(X)\n",
        "        clusters = [[i] for i in range(n)]\n",
        "        dist_matrix = self._calculate_distance_matrix(X)\n",
        "        self.merge_history = []\n",
        "        self.distances = []\n",
        "\n",
        "\n",
        "        while len(clusters) > 1:\n",
        "            min_distance = float('inf')\n",
        "            merge_i, merge_j = -1, -1\n",
        "\n",
        "            for i in range(len(clusters)):\n",
        "                for j in range(i+1, len(clusters)):\n",
        "                    distance = self._cluster_distance(clusters[i], clusters[j], X, dist_matrix)\n",
        "                    if distance < min_distance:\n",
        "                        min_distance = distance\n",
        "                        merge_i, merge_j = i, j\n",
        "\n",
        "            # Combinar os clusters mais próximos\n",
        "            # Salvar o novo cluster como uma lista de índices\n",
        "            new_cluster = clusters[merge_i] + clusters[merge_j]\n",
        "\n",
        "            # Salvar informações da fusão\n",
        "            self.merge_history.append((clusters[merge_i].copy(), clusters[merge_j].copy()))\n",
        "            self.distances.append(min_distance)\n",
        "\n",
        "            # Remover os clusters antigos e adicionar o novo\n",
        "            clusters = [clusters[k] for k in range(len(clusters)) if k != merge_i and k != merge_j]\n",
        "            clusters.append(new_cluster)\n",
        "\n",
        "linkage_methods = ['single', 'complete', 'average']\n",
        "\n",
        "print(\"Comparação da Sequência de Fusões (Distâncias) no X_simple:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for method in linkage_methods:\n",
        "    hc = HierarchicalClustering(linkage=method)\n",
        "    hc.fit(X_simple)\n",
        "\n",
        "    # Imprimir as 3 primeiras fusões e suas distâncias\n",
        "    print(f\"Método: {method.capitalize()} Linkage\")\n",
        "    for i in range(3):\n",
        "         # Os clusters são impressos como lista de índices (0 a 5)\n",
        "        c1_indices = hc.merge_history[i][0]\n",
        "        c2_indices = hc.merge_history[i][1]\n",
        "        print(f\"  Fusão {i+1}: Combina {c1_indices} e {c2_indices} (Distância: {hc.distances[i]:.3f})\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single: Prioriza a distância mínima, unindo P4 (índice 4) com P0/P1 rapidamente (P4 e P0 estão próximos de P1).\n",
        "\n",
        "Complete: Prioriza clusters mais compactos, unindo primeiro os pares mais distantes entre si no novo cluster (P0 e P1, P2 e P3).\n",
        "\n",
        "Average: Atua como um intermediário, utilizando a distância média de todos os pares de pontos."
      ],
      "metadata": {
        "id": "jOzMNwCoLOAT"
      },
      "id": "jOzMNwCoLOAT"
    },
    {
      "cell_type": "markdown",
      "id": "55661888",
      "metadata": {
        "id": "55661888"
      },
      "source": [
        "### Exercício 2: Análise do Dataset Wine - Seleção de Features e Comparação de Métodos\n",
        "\n",
        "Aplique a clusterização hierárquica do SciPy ao dataset Wine. Primeiro, você deve selecionar um bom par de features para visualização bidimensional, depois comparar diferentes métodos de ligação.\n",
        "\n",
        "**Tarefas:**\n",
        "1. Carregue o dataset Wine e explore suas features\n",
        "2. Selecione as duas melhores features para visualização (analise correlações, variâncias, etc.)\n",
        "3. Aplique os 4 métodos de ligação ('single', 'complete', 'average', 'ward') usando `scipy.cluster.hierarchy.linkage`\n",
        "4. Crie dendrogramas para cada método\n",
        "5. Determine visualmente qual método produz a melhor separação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9257b7",
      "metadata": {
        "id": "dc9257b7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "wine = load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "wine_df = pd.DataFrame(X_wine, columns=feature_names)\n",
        "wine_df['target'] = y_wine\n",
        "\n",
        "# A inspeção de pairplots (como visto no notebook K-Means) ou correlação/ANOVA\n",
        "# sugere que 'proline' e 'flavanoids' são altamente discriminatórios.\n",
        "# Features Selecionadas: Flavanoids e Proline (Índices 6 e 12)\n",
        "feature_indices = [6, 12] # 6: flavanoids, 12: proline\n",
        "X_selected_wine = X_wine[:, feature_indices]\n",
        "selected_feature_names = [feature_names[i] for i in feature_indices]\n",
        "\n",
        "# selecionando as duas melhores features (Flavanoids e Proline)\n",
        "print(\"Features Selecionadas para Visualização:\")\n",
        "print(f\"1. {selected_feature_names[0]}\")\n",
        "print(f\"2. {selected_feature_names[1]}\\n\")\n",
        "\n",
        "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    # Calcular a matriz de ligação\n",
        "    linkage_matrix = linkage(X_selected_wine, method=method)\n",
        "\n",
        "    dendrogram(linkage_matrix, ax=axes[i], no_labels=True, orientation='top')\n",
        "    axes[i].set_title(f'Dendrograma - {method.capitalize()}')\n",
        "    axes[i].set_xlabel('Pontos de Dados')\n",
        "    axes[i].set_ylabel('Distância')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ii7gm22sbpi",
      "metadata": {
        "id": "ii7gm22sbpi"
      },
      "source": [
        "### Exercício 3: Determinação do Número Ótimo de Clusters\n",
        "\n",
        "Com base no melhor método de ligação identificado no Exercício 2, determine o número ótimo de clusters para o dataset Wine usando análise visual do dendrograma e validação com os rótulos verdadeiros.\n",
        "\n",
        "**Tarefas:**\n",
        "1. Use o melhor método identificado no exercício anterior\n",
        "2. Crie um dendrograma detalhado com linha de corte ajustável\n",
        "3. Teste diferentes números de clusters (2, 3, 4, 5) usando `fcluster`\n",
        "4. Para cada número de clusters, visualize os clusters no scatter plot\n",
        "5. Determine o número ótimo de clusters justificando sua escolha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bkdb8je80qr",
      "metadata": {
        "id": "bkdb8je80qr"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import fcluster\n",
        "from scipy.stats import mode\n",
        "\n",
        "\n",
        "best_method = 'ward'\n",
        "\n",
        "\n",
        "linkage_matrix = linkage(X_selected_wine, method=best_method)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(linkage_matrix, labels=y_wine, leaf_rotation=90, color_threshold=100)\n",
        "plt.title(f'Dendrograma Detalhado (Método: {best_method.capitalize()})')\n",
        "plt.xlabel('Pontos de Dados (Rótulos Reais)')\n",
        "plt.ylabel('Distância')\n",
        "\n",
        "\n",
        "cut_height = linkage_matrix[len(linkage_matrix)-3, 2]\n",
        "plt.axhline(y=cut_height, color='r', linestyle='--', label=f'Corte para 3 clusters (Distância: {cut_height:.2f})')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "n_clusters_to_test = [2, 3, 4, 5]\n",
        "\n",
        "print(\"Análise do número ótimo de clusters:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "fig, axes = plt.subplots(1, len(n_clusters_to_test), figsize=(20, 5))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, k in enumerate(n_clusters_to_test):\n",
        "    # Obter K clusters (criterion='maxclust')\n",
        "    clusters = fcluster(linkage_matrix, k, criterion='maxclust')\n",
        "\n",
        "    axes[i].scatter(X_selected_wine[:, 0], X_selected_wine[:, 1], c=clusters, s=50, cmap='viridis', edgecolor='k')\n",
        "    axes[i].set_title(f'K={k} Clusters (Pureza: {calculate_purity(y_wine, clusters):.1%})')\n",
        "    axes[i].set_xlabel(selected_feature_names[0])\n",
        "    axes[i].set_ylabel(selected_feature_names[1])\n",
        "\n",
        "    # Imprimir a pureza para validação\n",
        "    print(f\"K={k}: Pureza (Taxa de Acerto) = {calculate_purity(y_wine, clusters):.1%}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}